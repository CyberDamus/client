# –ü–ª–∞–Ω –∑–∞–º–µ–Ω—ã MOCK_INTERPRETATIONS –Ω–∞ AI –º–æ–¥–µ–ª—å –¥–ª—è –¢–∞—Ä–æ

## üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –î–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥

### –§–ê–ó–ê 1: –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (–ù–∞—á–Ω–µ–º —Å —ç—Ç–æ–≥–æ) ‚úÖ

**–ú–æ–¥–µ–ª—å:** `barissglc/tinyllama-tarot-v1` (1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- ‚úÖ –ì–æ—Ç–æ–≤–∞—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é (—É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–∞ 5,769 —Ä–∞—Å–∫–ª–∞–¥–∞ –¢–∞—Ä–æ)
- ‚úÖ Open Source (Apache 2.0 –ª–∏—Ü–µ–Ω–∑–∏—è)
- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (CPU —Å 4GB RAM)
- ‚úÖ Inference: 1-3 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ CPU
- ‚úÖ –§–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞: 3 –∫–∞—Ä—Ç—ã ‚Üí –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:**
- üîó –ú–æ–¥–µ–ª—å: https://huggingface.co/barissglc/tinyllama-tarot-v1
- üîó –î–∞—Ç–∞—Å–µ—Ç (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è): https://huggingface.co/datasets/barissglc/tarot (5,769 –ø—Ä–∏–º–µ—Ä–æ–≤)
- üîó –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: https://huggingface.co/datasets/Dendory/tarot (5,770 –ø—Ä–∏–º–µ—Ä–æ–≤)

**–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
- VPS —Å 4-8GB RAM (–±–µ–∑ GPU) - ~$10-20/–º–µ—Å
- –ü—Ä–∏–º–µ—Ä—ã: DigitalOcean ($12/–º–µ—Å), Hetzner CPX21 ($7/–º–µ—Å)
- Ollama –¥–ª—è –¥–µ–ø–ª–æ—è (—Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
Next.js Client ‚Üí Node.js API Server ‚Üí Ollama ‚Üí TinyLlama-Tarot
                 (lib/api/tarot.ts)     (localhost:11434)
```

**–ß—Ç–æ —Å–¥–µ–ª–∞–µ–º:**
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama –Ω–∞ —Å–µ—Ä–≤–µ—Ä
2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å `barissglc/tinyllama-tarot-v1` –≤ GGUF (4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ)
3. –°–æ–∑–¥–∞—Ç—å Modelfile –¥–ª—è Ollama —Å –ø—Ä–æ–º–ø—Ç–æ–º –¥–ª—è –¢–∞—Ä–æ
4. –°–æ–∑–¥–∞—Ç—å API endpoint `/api/tarot/interpret` (POST)
   - Input: 3 –∫–∞—Ä—Ç—ã + –ø–æ–∑–∏—Ü–∏–∏ (Past/Present/Future) + query (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
   - Output: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤ —Å—Ç–∏–ª–µ CyberDamus
5. –ó–∞–º–µ–Ω–∏—Ç—å `generateMockInterpretation()` –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤
6. –î–æ–±–∞–≤–∏—Ç—å error handling –∏ retry –ª–æ–≥–∏–∫—É
7. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ó–∞–ø—É—Å—Ç–∏–º –∑–∞ 1-2 —á–∞—Å–∞
- –ù–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
- –°—Ä–∞–∑—É —É–≤–∏–¥–∏–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
- –õ–µ–≥–∫–æ –∑–∞–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ–∑–∂–µ

---

### –§–ê–ó–ê 2: –ê–ø–≥—Ä–µ–π–¥ (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –∑–∞—Ö–æ—Ç–∏–º —É–ª—É—á—à–∏—Ç—å) üöÄ

**–ú–æ–¥–µ–ª—å:** Fine-tuned Qwen2.5-1.5B –∏–ª–∏ Llama 3.2-3B
- üî• –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (2025)
- üî• –í—ã—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π
- üî• 1.5B –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å 7B –º–æ–¥–µ–ª—è–º–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
- üî• Inference: 2-4 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ CPU

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏):**
1. **Qwen2.5-1.5B** - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è, –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å 7B –º–æ–¥–µ–ª—è–º–∏
2. **Llama 3.2-3B** - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
3. **Phi-3 (3.8B)** - –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Å–µ 3B
4. **MobileLLaMA-2.7B** - –Ω–∞ 40% –±—ã—Å—Ç—Ä–µ–µ TinyLlama

**–ü—Ä–æ—Ü–µ—Å—Å:**
1. Fine-tune –Ω–∞ Google Colab (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π T4 GPU, 3-6 —á–∞—Å–æ–≤ –æ–±—É—á–µ–Ω–∏—è)
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç `barissglc/tarot` (—Ç–µ –∂–µ 5,769 –ø—Ä–∏–º–µ—Ä–æ–≤)
3. LoRA/QLoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
4. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤ 4-bit (GGUF Q4_K_M)
5. –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ Ollama (–∑–∞–º–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏)

**–ö–æ–≥–¥–∞ –¥–µ–ª–∞—Ç—å:**
- –ü–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –§–∞–∑—ã 1
- –ï—Å–ª–∏ —Ç–µ–∫—É—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–µ —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç
- –ö–æ–≥–¥–∞ –±—É–¥–µ—Ç –≤—Ä–µ–º—è –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

---

## üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –§–ê–ó–´ 1

### 1. Backend Setup

#### 1.1 –°–æ–∑–¥–∞—Ç—å —Å–µ—Ä–≤–µ—Ä–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
```bash
mkdir -p server/src/{routes,controllers,services}
cd server
npm init -y
npm install express typescript @types/node @types/express cors dotenv ollama
npm install -D nodemon ts-node
```

#### 1.2 –ù–∞—Å—Ç—Ä–æ–∏—Ç—å TypeScript
–°–æ–∑–¥–∞—Ç—å `server/tsconfig.json`:
```json
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true
  }
}
```

#### 1.3 –°–æ–∑–¥–∞—Ç—å API endpoint
–§–∞–π–ª: `server/src/routes/tarot.ts`
```typescript
import express from 'express';
import { interpretTarotReading } from '../controllers/tarotController';

const router = express.Router();
router.post('/interpret', interpretTarotReading);

export default router;
```

–§–∞–π–ª: `server/src/controllers/tarotController.ts`
```typescript
import { Request, Response } from 'express';
import { generateTarotInterpretation } from '../services/ollamaService';

export async function interpretTarotReading(req: Request, res: Response) {
  try {
    const { cards, query } = req.body;
    const interpretation = await generateTarotInterpretation(cards, query);
    res.json({ interpretation });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate interpretation' });
  }
}
```

–§–∞–π–ª: `server/src/services/ollamaService.ts`
```typescript
import ollama from 'ollama';

interface TarotCard {
  id: number;
  inverted: boolean;
}

export async function generateTarotInterpretation(
  cards: TarotCard[],
  query?: string
): Promise<string> {
  const prompt = buildPrompt(cards, query);

  const response = await ollama.chat({
    model: 'cyberdamus-tarot',
    messages: [{ role: 'user', content: prompt }],
    options: {
      temperature: 0.8,
      top_p: 0.9,
    }
  });

  return response.message.content;
}

function buildPrompt(cards: TarotCard[], query?: string): string {
  // TODO: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ä—Ç
  return `...`;
}
```

---

### 2. Ollama Setup

#### 2.1 –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å
ollama serve
```

#### 2.2 –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ GGUF

**–í–∞—Ä–∏–∞–Ω—Ç A: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HuggingFace Space (–õ–ï–ì–ö–û)**
1. –û—Ç–∫—Ä—ã—Ç—å https://huggingface.co/spaces/ggml-org/gguf-my-repo
2. –í–≤–µ—Å—Ç–∏: `barissglc/tinyllama-tarot-v1`
3. –í—ã–±—Ä–∞—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ: `Q4_K_M` (4-bit, –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å)
4. –°–∫–∞—á–∞—Ç—å GGUF —Ñ–∞–π–ª

**–í–∞—Ä–∏–∞–Ω—Ç B: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ**
```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å
huggingface-cli download barissglc/tinyllama-tarot-v1 --local-dir ./models/tinyllama-tarot

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ GGUF
python convert_hf_to_gguf.py ./models/tinyllama-tarot

# –ö–≤–∞–Ω—Ç–æ–≤–∞—Ç—å –≤ 4-bit
./llama-quantize ./models/tinyllama-tarot/ggml-model-f16.gguf \
                 ./models/tinyllama-tarot/ggml-model-q4_k_m.gguf Q4_K_M
```

#### 2.3 –°–æ–∑–¥–∞—Ç—å Modelfile

–§–∞–π–ª: `Modelfile`
```
FROM ./tinyllama-tarot-q4_k_m.gguf

TEMPLATE """{{ .System }}

User: {{ .Prompt }}
Assistant:"""

SYSTEM """You are CyberDamus, a mystical AI oracle specializing in Tarot readings.
Your interpretations blend ancient wisdom with cyberpunk aesthetics.
You provide insightful, personalized readings based on the cards drawn.

Guidelines:
- Be mystical yet practical
- Use cyberpunk-themed language when appropriate
- Focus on Past, Present, Future structure
- Consider card orientations (upright/inverted)
- Be concise but meaningful (2-3 paragraphs)
- Match the CyberDamus aesthetic and tone"""

PARAMETER temperature 0.8
PARAMETER top_p 0.9
PARAMETER top_k 40
```

#### 2.4 –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ Ollama
```bash
ollama create cyberdamus-tarot -f Modelfile
ollama list  # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
```

#### 2.5 –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
```bash
ollama run cyberdamus-tarot "Give me a tarot reading for cards: The Fool, The Magician, The High Priestess"
```

---

### 3. Client Integration

#### 3.1 –°–æ–∑–¥–∞—Ç—å API –∫–ª–∏–µ–Ω—Ç

–§–∞–π–ª: `lib/api/tarotClient.ts`
```typescript
interface TarotCard {
  id: number;
  inverted: boolean;
}

interface InterpretationRequest {
  cards: TarotCard[];
  query?: string;
}

interface InterpretationResponse {
  interpretation: string;
}

export async function generateInterpretation(
  cards: TarotCard[],
  query?: string
): Promise<string> {
  const response = await fetch('/api/tarot/interpret', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ cards, query }),
  });

  if (!response.ok) {
    throw new Error('Failed to generate interpretation');
  }

  const data: InterpretationResponse = await response.json();
  return data.interpretation;
}
```

#### 3.2 –û–±–Ω–æ–≤–∏—Ç—å lib/store.ts

–ó–∞–º–µ–Ω–∏—Ç—å:
```typescript
// –°–¢–ê–†–´–ô –ö–û–î (–£–î–ê–õ–ò–¢–¨)
import { MOCK_INTERPRETATIONS } from './mockData'

export function generateMockInterpretation(): string {
  const randomIndex = Math.floor(Math.random() * MOCK_INTERPRETATIONS.length)
  return MOCK_INTERPRETATIONS[randomIndex]
}
```

–ù–∞:
```typescript
// –ù–û–í–´–ô –ö–û–î
import { generateInterpretation } from './api/tarotClient'

export async function generateAIInterpretation(
  cards: ParsedCard[],
  query?: string
): Promise<string> {
  try {
    return await generateInterpretation(cards, query);
  } catch (error) {
    console.error('AI interpretation failed:', error);
    // Fallback to mock if AI fails
    return generateFallbackInterpretation();
  }
}

function generateFallbackInterpretation(): string {
  return "The cosmic circuits align mysteriously. Your reading reveals a path through the digital realm...";
}
```

#### 3.3 –û–±–Ω–æ–≤–∏—Ç—å lib/solana/mint.ts

–í —Ñ—É–Ω–∫—Ü–∏–∏ `mintFortuneTokenWithRetry`, –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –º–∏–Ω—Ç–∏–Ω–≥–∞:
```typescript
// –ë–´–õ–û:
// const interpretation = generateMockInterpretation()

// –°–¢–ê–õ–û:
const interpretation = await generateAIInterpretation(cards, query)
```

#### 3.4 –î–æ–±–∞–≤–∏—Ç—å loading states

–í `app/page.tsx`:
```typescript
const [isGeneratingInterpretation, setIsGeneratingInterpretation] = useState(false)

// –í handleDrawCards:
setIsGeneratingInterpretation(true)
try {
  const result = await mintFortuneTokenWithRetry(...)
  // ...
} finally {
  setIsGeneratingInterpretation(false)
}
```

---

### 4. Testing & Optimization

#### 4.1 –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ 10+ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—è—Ö –∫–∞—Ä—Ç
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1-3 —Å–µ–∫)
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å query –∏ –±–µ–∑ query
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å upright –∏ inverted –∫–∞—Ä—Ç—ã
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å error handling

#### 4.2 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å SYSTEM –ø—Ä–æ–º–ø—Ç –¥–ª—è CyberDamus —Å—Ç–∏–ª—è
- –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã (few-shot learning) –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å temperature (0.7-0.9)

#### 4.3 Performance
- –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–∞
- –î–æ–±–∞–≤–∏—Ç—å rate limiting –¥–ª—è API
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞

#### 4.4 Error Handling
- Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
- Fallback –Ω–∞ mock –ø—Ä–∏ —Å–±–æ–µ AI
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
- User-friendly —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö

---

## üí∞ –û—Ü–µ–Ω–∫–∞ –∑–∞—Ç—Ä–∞—Ç

### –í—Ä–µ–º—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:
- **Backend Setup**: 1-2 —á–∞—Å–∞
- **Ollama Setup**: 1-2 —á–∞—Å–∞
- **Client Integration**: 1-2 —á–∞—Å–∞
- **Testing & Optimization**: 1-2 —á–∞—Å–∞
- **–ò–¢–û–ì–û –§–ê–ó–ê 1**: 4-8 —á–∞—Å–æ–≤

### –î–µ–Ω—å–≥–∏:
- **–§–∞–∑–∞ 1**: $7-20/–º–µ—Å (VPS CPU-only)
  - Hetzner CPX21: ‚Ç¨7.05/–º–µ—Å (4 vCPU, 8GB RAM)
  - DigitalOcean Basic: $12/–º–µ—Å (2 vCPU, 4GB RAM)
  - Contabo VPS M: ‚Ç¨8.99/–º–µ—Å (6 vCPU, 16GB RAM)

- **–§–∞–∑–∞ 2**: $0 (Google Colab –±–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è) + —Ç–æ—Ç –∂–µ VPS

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- **TinyLlama 1.1B (4-bit)**:
  - –†–∞–∑–º–µ—Ä: ~637 MB RAM
  - CPU inference: 1-3 —Å–µ–∫—É–Ω–¥—ã
  - Tokens/sec: 15-30 –Ω–∞ CPU (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç CPU)

- **Qwen2.5-1.5B (4-bit) [–§–∞–∑–∞ 2]**:
  - –†–∞–∑–º–µ—Ä: ~900 MB RAM
  - CPU inference: 2-4 —Å–µ–∫—É–Ω–¥—ã
  - –í—ã—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ —Å—Ö–æ–∂–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏

---

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

**–ú–∏–Ω–∏–º—É–º (–¥–ª—è –§–∞–∑—ã 1):**
- CPU: 2+ cores
- RAM: 4GB (2GB –¥–ª—è —Å–∏—Å—Ç–µ–º—ã + 637MB –¥–ª—è –º–æ–¥–µ–ª–∏ + –±—É—Ñ–µ—Ä)
- –î–∏—Å–∫: 5GB
- –û–°: Linux (Ubuntu/Debian)

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:**
- CPU: 4+ cores (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π x86_64)
- RAM: 8GB
- –î–∏—Å–∫: 10GB SSD
- –û–°: Ubuntu 22.04 LTS

**–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–¥–ª—è –§–∞–∑—ã 2):**
- GPU: 4-8GB VRAM (—É—Å–∫–æ—Ä–∏—Ç inference –≤ 5-10x)
- –ù–∞–ø—Ä–∏–º–µ—Ä: GTX 1060 6GB, RTX 3060 12GB

### –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏

**Input (–∫ –º–æ–¥–µ–ª–∏):**
```
System: You are CyberDamus...

User: Give me a tarot reading for these cards:
- Past: The Fool (upright)
- Present: The Magician (inverted)
- Future: The High Priestess (upright)

Query: "What does my career future hold?"